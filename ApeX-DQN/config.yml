# Environment parameters

env_name: PongNoFrameskip-v4 #LunarLander-v2
atari: True
learner_device: cpu
worker_device: cpu
random_seed: 2019
num_workers: 1
num_learners: 1

# Training parameters
model: Apex-DQN
network_type: DQN
batch_size: 32
max_num_updates: 5 # number of episodes from all agents
max_ep_length: 300 # maximum number of steps per episode
buffer_max_size: 100000 # maximum capacity of replay memory
use_per: True  # use prioritized experience replay
priority_alpha: 0.6 # controls the randomness vs prioritisation of the prioritised sampling (0.0 = Uniform sampling, 1.0 = Greedy prioritisation)
priority_beta: 0.4
priority_beta_start: 0.4 # starting value of beta - controls to what degree IS weights influence the gradient updates to correct for the bias introduces by priority sampling (0 - no correction, 1 - full correction)
priority_beta_end: 1.0 # beta will be linearly annelaed from its start value to this value thoughout training
gamma: 0.99 # Discount rate (gamma) for future rewards
unroll_steps: 1 # number of future steps to collect experiences for N-step returns
num_step: 1
eps_greedy: 0.2
eps_decay: 0.95
worker_buffer_size: 100

# Network parameters
tau: 0.01
q_lr: 0.003
policy_lr: 0.003


# Miscellaneous
results_path: results