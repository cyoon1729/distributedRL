# Environment parameters

env_name: PongNoFrameskip-v4 # LunarLander-v2 #  CartPole-v1 # 
atari: True
learner_device: cpu
worker_device: cpu
random_seed: 2019
num_workers: 16
num_learners: 1

# Training parameters
model: Apex-DQN
batch_size: 128
max_num_updates: 100000 # number of episodes from all agents
max_ep_length: 300 # maximum number of steps per episode
buffer_max_size: 500000 # maximum capacity of replay memory
use_per: True  # use prioritized experience replay
priority_alpha: 0.6 # controls the randomness vs prioritisation of the prioritised sampling (0.0 = Uniform sampling, 1.0 = Greedy prioritisation)
priority_beta: 0.4
priority_beta_start: 0.4 # starting value of beta - controls to what degree IS weights influence the gradient updates to correct for the bias introduces by priority sampling (0 - no correction, 1 - full correction)
priority_beta_end: 1.0 # beta will be linearly annelaed from its start value to this value thoughout training
gamma: 0.99 # Discount rate (gamma) for future rewards
num_step: 5
eps_greedy: 0.2
eps_decay: 0.90
worker_buffer_size: 1000

# Network parameters
tau: 0.01
learning_rate: 0.0003

# Miscellaneous
results_path: results