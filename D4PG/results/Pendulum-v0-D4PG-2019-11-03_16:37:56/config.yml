# Environment parameters

env: Pendulum-v0 #LunarLander-v2 
device: cpu
random_seed: 2019
num_agents: 1

# Training parameters
model: D4PG
network_type: QuantileDQN
batch_size: 32
num_train_steps: 20000 # number of episodes from all agents
max_ep_length: 300 # maximum number of steps per episode
buffer_max_size: 100000 # maximum capacity of replay memory
use_per: True  # use prioritized experience replay
priority_alpha: 0.6 # controls the randomness vs prioritisation of the prioritised sampling (0.0 = Uniform sampling, 1.0 = Greedy prioritisation)
priority_beta_start: 0.4 # starting value of beta - controls to what degree IS weights influence the gradient updates to correct for the bias introduces by priority sampling (0 - no correction, 1 - full correction)
priority_beta_end: 1.0 # beta will be linearly annelaed from its start value to this value thoughout training
gamma: 0.99 # Discount rate (gamma) for future rewards
unroll_steps: 1 # number of future steps to collect experiences for N-step returns
num_quantiles: 4 # number of quantiles for quantile regression dqn

# Network parameters
tau: 0.01
q_lr: 0.003
policy_lr: 0.003


# Miscellaneous
results_path: results

